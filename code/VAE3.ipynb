{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aebd460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Anaconda3\\envs\\DeepSumMoon\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "\n",
    "# 1. 多模态数据集类（支持样本索引和标签）\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_data=None, text_data=None, vector_data=None, targets=None, is_test=False, device=device):\n",
    "        self.image_data = image_data\n",
    "        self.text_data = text_data\n",
    "        self.vector_data = vector_data\n",
    "        self.targets = targets  # 标签（y）\n",
    "        self.is_test = is_test\n",
    "        self.device = device\n",
    "        \n",
    "        # 确定样本数量\n",
    "        if image_data is not None:\n",
    "            self.num_samples = len(image_data)\n",
    "        elif text_data is not None:\n",
    "            self.num_samples = len(text_data)\n",
    "        else:\n",
    "            self.num_samples = len(vector_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'index': idx}  # 样本索引\n",
    "        if self.image_data is not None:\n",
    "            sample['image'] = torch.tensor(self.image_data[idx], dtype=torch.float32).to(self.device)\n",
    "        if self.text_data is not None:\n",
    "            sample['text'] = torch.tensor(self.text_data[idx], dtype=torch.float32).to(self.device)\n",
    "        if self.vector_data is not None:\n",
    "            sample['vector'] = torch.tensor(self.vector_data[idx], dtype=torch.float32).to(self.device)\n",
    "        if self.targets is not None:\n",
    "            sample['target'] = torch.tensor(self.targets[idx], dtype=torch.float32).to(self.device)  # 标签\n",
    "        return sample\n",
    "\n",
    "\n",
    "# 2. 模态编码器（保持不变）\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3)\n",
    "        )\n",
    "        self.spatial_attn = nn.Sequential(\n",
    "            nn.Conv2d(3, 1, kernel_size=1, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc = nn.Linear(3 * 8 * 8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block1(x) + x\n",
    "        x2 = self.conv_block2(x1) + F.interpolate(x1, size=x1.shape[-1]//2)\n",
    "        x3 = self.conv_block3(x2) + F.interpolate(x2, size=x2.shape[-1]//2)\n",
    "        attn = self.spatial_attn(x3)\n",
    "        x3 = x3 * attn\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        return self.fc(x3)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=50, hidden_dim=64, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.text_attn = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        attn_weights = F.softmax(self.text_attn(out).squeeze(-1), dim=1)\n",
    "        weighted_out = torch.bmm(attn_weights.unsqueeze(1), out).squeeze(1)\n",
    "        return self.fc(weighted_out)\n",
    "\n",
    "\n",
    "class VectorEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=32, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# 3. 多模态融合模型（保持不变）\n",
    "class MultimodalEncoder(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, vector_encoder, latent_dim=128, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vector_encoder = vector_encoder\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.modal_weight = nn.Parameter(torch.ones(3))\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: dict, return_embedding=False):\n",
    "        batch_size = next(iter(v for k, v in x.items() if k != 'index')).size(0)\n",
    "        \n",
    "        img_feat = self.image_encoder(x['image']) if 'image' in x else torch.zeros(batch_size, 128).to(device)\n",
    "        text_feat = self.text_encoder(x['text']) if 'text' in x else torch.zeros(batch_size, 128).to(device)\n",
    "        vec_feat = self.vector_encoder(x['vector']) if 'vector' in x else torch.zeros(batch_size, 128).to(device)\n",
    "        \n",
    "        img_feat_expand = img_feat.unsqueeze(1)\n",
    "        text_feat_expand = text_feat.unsqueeze(1)\n",
    "        vec_feat_expand = vec_feat.unsqueeze(1)\n",
    "        \n",
    "        text_attended, _ = self.cross_attn(text_feat_expand, img_feat_expand, img_feat_expand)\n",
    "        vec_attended, _ = self.cross_attn(vec_feat_expand, img_feat_expand, img_feat_expand)\n",
    "        \n",
    "        weights = F.softmax(self.modal_weight, dim=0)\n",
    "        img_feat_weighted = img_feat * weights[0]\n",
    "        text_feat_weighted = text_attended.squeeze(1) * weights[1]\n",
    "        vec_feat_weighted = vec_attended.squeeze(1) * weights[2]\n",
    "        \n",
    "        fused = torch.cat([img_feat_weighted, text_feat_weighted, vec_feat_weighted], dim=1)\n",
    "        unified_embedding = F.relu(self.fusion(fused))  # 统一表征（特征）\n",
    "        output = self.regressor(unified_embedding)\n",
    "        projection = self.projection(unified_embedding)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return output, projection, unified_embedding\n",
    "        return output, projection\n",
    "\n",
    "\n",
    "# 4. 联邦学习客户端（支持训练集+测试集特征提取，含标签）\n",
    "class Client:\n",
    "    def __init__(self, client_id, model, train_dataset, test_dataset=None, learning_rate=0.001):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model).to(device)\n",
    "        self.train_dataset = train_dataset  # 本地训练集\n",
    "        self.test_dataset = test_dataset    # 本地测试集\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        self.train_feature_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # 提取特征用（不打乱）\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) if test_dataset else None\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.local_epochs = 3\n",
    "        \n",
    "        # 互信息映射函数（保持不变）\n",
    "        self.f_map = nn.Sequential(\n",
    "            nn.Linear(model.latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        for param in self.f_map.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 损失函数（保持不变）\n",
    "    def compute_mi_loss_Y_Z(self, Z, Y):\n",
    "        f_Z = self.f_map(Z)\n",
    "        f_Z_norm = (f_Z - f_Z.mean(dim=0)) / (f_Z.std(dim=0) + 1e-8)\n",
    "        Y_norm = (Y - Y.mean(dim=0)) / (Y.std(dim=0) + 1e-8)\n",
    "        cov = torch.cov(torch.cat([f_Z_norm, Y_norm], dim=1).T)[0, 1]\n",
    "        corr = cov / (torch.std(f_Z_norm) * torch.std(Y_norm) + 1e-8)\n",
    "        return -torch.log(torch.abs(corr) + 1e-8)\n",
    "\n",
    "    def compute_kl_loss_modal(self, modal_feats):\n",
    "        if len(modal_feats) < 2:\n",
    "            return 0.0\n",
    "        kl_total = 0.0\n",
    "        num_pairs = 0\n",
    "        var = nn.Parameter(torch.tensor(0.1)).to(device)\n",
    "        dist_list = [torch.distributions.Normal(feat, var) for feat in modal_feats]\n",
    "        \n",
    "        for i in range(len(dist_list)):\n",
    "            for j in range(i+1, len(dist_list)):\n",
    "                kl_ij = torch.distributions.kl.kl_divergence(dist_list[i], dist_list[j]).mean()\n",
    "                kl_ji = torch.distributions.kl.kl_divergence(dist_list[j], dist_list[i]).mean()\n",
    "                kl_total += (kl_ij + kl_ji) / 2\n",
    "                num_pairs += 1\n",
    "        return kl_total / num_pairs\n",
    "\n",
    "    def compute_contrastive_loss_anti_forget(self, current_Z, prev_global_Z, history_global_Zs, temperature=0.5):\n",
    "        current_Z_norm = F.normalize(current_Z, dim=1)\n",
    "        prev_global_Z_norm = F.normalize(prev_global_Z, dim=1)\n",
    "        \n",
    "        pos_samples = prev_global_Z_norm\n",
    "        if history_global_Zs:\n",
    "            neg_samples = torch.cat([F.normalize(emb, dim=1) for emb in history_global_Zs], dim=0)\n",
    "        else:\n",
    "            neg_samples = current_Z_norm[torch.randperm(current_Z_norm.size(0))]\n",
    "        \n",
    "        pos_sim = torch.matmul(current_Z_norm, pos_samples.T).diag() / temperature\n",
    "        neg_sim = torch.matmul(current_Z_norm, neg_samples.T) / temperature\n",
    "        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def local_train(self, global_model, prev_global_Z, history_global_Zs, mu=0.01):\n",
    "        \"\"\"本地训练\"\"\"\n",
    "        self.model.load_state_dict(global_model.state_dict())\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for epoch in range(self.local_epochs):\n",
    "            for batch in self.train_dataloader:\n",
    "                current_pred, _, current_Z = self.model(batch, return_embedding=True)\n",
    "                \n",
    "                modal_feats = []\n",
    "                if 'image' in batch:\n",
    "                    modal_feats.append(self.model.image_encoder(batch['image']))\n",
    "                if 'text' in batch:\n",
    "                    modal_feats.append(self.model.text_encoder(batch['text']))\n",
    "                if 'vector' in batch:\n",
    "                    modal_feats.append(self.model.vector_encoder(batch['vector']))\n",
    "                \n",
    "                reg_loss = F.mse_loss(current_pred.squeeze(), batch['target'])\n",
    "                mi_loss = self.compute_mi_loss_Y_Z(current_Z, batch['target'].unsqueeze(1))\n",
    "                kl_loss = self.compute_kl_loss_modal(modal_feats)\n",
    "                contrast_loss = self.compute_contrastive_loss_anti_forget(\n",
    "                    current_Z, prev_global_Z, history_global_Zs\n",
    "                )\n",
    "                \n",
    "                loss = reg_loss + mu * (mi_loss + kl_loss + contrast_loss)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / (self.local_epochs * len(self.train_dataloader))\n",
    "        print(f\"客户端 {self.client_id} 平均损失: {avg_loss:.4f}\")\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def extract_train_features(self, use_global_model=False, global_model=None):\n",
    "        \"\"\"提取训练集特征，与对应标签（y）一起返回\"\"\"\n",
    "        # 选择模型（全局或本地）\n",
    "        model = global_model if (use_global_model and global_model) else self.model\n",
    "        model.eval()\n",
    "        all_features = []\n",
    "        all_indices = []  # 训练样本索引\n",
    "        all_targets = []  # 训练样本标签（y）\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.train_feature_dataloader:\n",
    "                _, _, emb = model(batch, return_embedding=True)\n",
    "                all_features.append(emb.cpu())\n",
    "                all_indices.extend(batch['index'].cpu().numpy())\n",
    "                all_targets.extend(batch['target'].cpu().numpy())  # 收集标签\n",
    "        \n",
    "        model.train()\n",
    "        # 合并结果\n",
    "        features_np = torch.cat(all_features, dim=0).numpy()\n",
    "        targets_np = np.array(all_targets)\n",
    "        \n",
    "        return {\n",
    "            'client_id': self.client_id,\n",
    "            'train_indices': all_indices,\n",
    "            'train_features': features_np,  # 训练集特征 (N, 128)\n",
    "            'train_targets': targets_np     # 对应标签 (N,)\n",
    "        }\n",
    "\n",
    "    def extract_test_features(self, use_global_model=False, global_model=None):\n",
    "        \"\"\"提取测试集特征\"\"\"\n",
    "        if not self.test_dataset:\n",
    "            return None\n",
    "        \n",
    "        model = global_model if (use_global_model and global_model) else self.model\n",
    "        model.eval()\n",
    "        all_features = []\n",
    "        all_indices = []\n",
    "        all_preds = []\n",
    "        all_targets = []  # 测试集标签（如有）\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_dataloader:\n",
    "                pred, _, emb = model(batch, return_embedding=True)\n",
    "                all_features.append(emb.cpu())\n",
    "                all_indices.extend(batch['index'].cpu().numpy())\n",
    "                all_preds.append(pred.squeeze().cpu().numpy())\n",
    "                if 'target' in batch:\n",
    "                    all_targets.extend(batch['target'].cpu().numpy())  # 测试集标签\n",
    "        \n",
    "        model.train()\n",
    "        features_np = torch.cat(all_features, dim=0).numpy()\n",
    "        preds_np = np.concatenate(all_preds, axis=0)\n",
    "        targets_np = np.array(all_targets) if all_targets else None\n",
    "        \n",
    "        return {\n",
    "            'client_id': self.client_id,\n",
    "            'test_indices': all_indices,\n",
    "            'test_features': features_np,  # 测试集特征 (N, 128)\n",
    "            'test_preds': preds_np,         # 预测值 (N,)\n",
    "            'test_targets': targets_np      # 测试集标签（如有）\n",
    "        }\n",
    "\n",
    "\n",
    "# 5. 联邦学习服务器（协调训练集+测试集特征提取）\n",
    "class Server:\n",
    "    def __init__(self, model, num_clients):\n",
    "        self.global_model = copy.deepcopy(model).to(device)\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.history_global_embs = []\n",
    "        self.latent_dim = model.latent_dim\n",
    "\n",
    "    def add_client(self, client):\n",
    "        self.clients.append(client)\n",
    "\n",
    "    def aggregate_parameters(self, client_params_list):\n",
    "        \"\"\"参数聚合（保持不变）\"\"\"\n",
    "        aggregated_params = {\n",
    "            name: torch.zeros_like(param) \n",
    "            for name, param in self.global_model.state_dict().items()\n",
    "        }\n",
    "        \n",
    "        for params in client_params_list:\n",
    "            for name, param in params.items():\n",
    "                if 'num_batches_tracked' in name:\n",
    "                    aggregated_params[name] += param.long() // self.num_clients\n",
    "                else:\n",
    "                    aggregated_params[name] += param.to(aggregated_params[name].dtype) / self.num_clients\n",
    "        \n",
    "        self.global_model.load_state_dict(aggregated_params)\n",
    "        return self.global_model.state_dict()\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        \"\"\"全局模型评估（保持不变）\"\"\"\n",
    "        if test_dataset is None:\n",
    "            return\n",
    "        self.global_model.eval()\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        total_mse = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                pred, _ = self.global_model(batch)\n",
    "                mse = F.mse_loss(pred.squeeze(), batch['target'], reduction='sum')\n",
    "                total_mse += mse.item()\n",
    "                total_samples += batch['target'].size(0)\n",
    "        \n",
    "        rmse = np.sqrt(total_mse / total_samples)\n",
    "        #print(f\"全局模型测试RMSE: {rmse:.4f}\")\n",
    "        self.global_model.train()\n",
    "        return rmse\n",
    "\n",
    "    def federated_train(self, rounds=5, global_test_dataset=None):\n",
    "        \"\"\"联邦训练主流程\"\"\"\n",
    "        for round_idx in range(rounds):\n",
    "            print(f\"\\n===== 联邦轮次 {round_idx + 1}/{rounds} =====\")\n",
    "            \n",
    "            client_params_list = []\n",
    "            \n",
    "            # 获取上一轮全局表征\n",
    "            if global_test_dataset and self.history_global_embs:\n",
    "                test_loader = DataLoader(global_test_dataset, batch_size=32, shuffle=False)\n",
    "                test_batch = next(iter(test_loader))\n",
    "                with torch.no_grad():\n",
    "                    _, _, prev_global_Z = self.global_model(test_batch, return_embedding=True)\n",
    "                prev_global_Z = prev_global_Z.detach()\n",
    "                history_global_Zs = [z.detach() for z in self.history_global_embs[-3:]]\n",
    "            else:\n",
    "                prev_global_Z = torch.zeros(32, self.latent_dim).to(device)\n",
    "                history_global_Zs = []\n",
    "            \n",
    "            # 客户端本地训练\n",
    "            for client in self.clients:\n",
    "                client_params = client.local_train(\n",
    "                    self.global_model, prev_global_Z, history_global_Zs\n",
    "                )\n",
    "                client_params_list.append(client_params)\n",
    "            \n",
    "            # 聚合参数\n",
    "            self.aggregate_parameters(client_params_list)\n",
    "            \n",
    "            # 评估\n",
    "            self.evaluate(global_test_dataset)\n",
    "        \n",
    "        return self.global_model\n",
    "\n",
    "    def get_all_client_features(self, use_global_model=True):\n",
    "        \"\"\"获取所有客户端的训练集+测试集特征（含标签）\"\"\"\n",
    "        all_train_features = []\n",
    "        all_test_features = []\n",
    "        \n",
    "        for client in self.clients:\n",
    "            # 提取训练集特征（含标签）\n",
    "            train_feats = client.extract_train_features(\n",
    "                use_global_model=use_global_model,\n",
    "                global_model=self.global_model if use_global_model else None\n",
    "            )\n",
    "            all_train_features.append(train_feats)\n",
    "            \n",
    "            # 提取测试集特征\n",
    "            test_feats = client.extract_test_features(\n",
    "                use_global_model=use_global_model,\n",
    "                global_model=self.global_model if use_global_model else None\n",
    "            )\n",
    "            if test_feats:\n",
    "                all_test_features.append(test_feats)\n",
    "        \n",
    "        return all_train_features, all_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bd0635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "客户端 1 - 训练样本: 666, 测试样本: 166\n",
      "客户端 2 - 训练样本: 666, 测试样本: 166\n",
      "客户端 3 - 训练样本: 668, 测试样本: 168\n"
     ]
    }
   ],
   "source": [
    "def generate_sim_data(num_samples=1000):\n",
    "    image_data = np.random.randn(num_samples, 3, 32, 32)#xpd矩阵\n",
    "    text_data = np.random.randn(num_samples, 10, 50)\n",
    "    vector_data = np.random.randn(num_samples, 32)\n",
    "    img=image_data \n",
    "    txt=text_data\n",
    "    vec=vector_data\n",
    "    targets =8* 2 / (\n",
    "            np.exp(\n",
    "                0.02 * np.sum(img.reshape(img.shape[0], -1)[:, ::10], axis=1)\n",
    "                + 0.02 * np.sum(txt.reshape(txt.shape[0], -1)[:, ::10], axis=1)\n",
    "                + 0.02 * np.sum(vec[:, ::10], axis=1)\n",
    "            ) + np.exp(\n",
    "                - (\n",
    "                    0.02 * np.sum(img.reshape(img.shape[0], -1)[:, ::10], axis=1)\n",
    "                    + 0.02 * np.sum(txt.reshape(txt.shape[0], -1)[:, ::10], axis=1)\n",
    "                    + 0.02 * np.sum(vec[:, ::10], axis=1))))\n",
    "    return image_data, text_data, vector_data, targets\n",
    "\n",
    "# 生成数据\n",
    "train_image, train_text, train_vector, train_target = generate_sim_data(2000)\n",
    "global_test_image, global_test_text, global_test_vector, global_test_target = generate_sim_data(500)\n",
    "\n",
    "# 分配客户端数据\n",
    "import numpy as np  # 需导入numpy用于生成随机索引\n",
    "\n",
    "# 分配客户端数据\n",
    "client_train_datasets = []\n",
    "client_test_datasets = []\n",
    "\n",
    "# ---------------------- 训练集随机分配 ----------------------\n",
    "# 计算训练集总样本数\n",
    "n_train = len(train_image)  # 原代码中总训练样本为2000（可自动适配实际长度）\n",
    "# 计算每个客户端的训练样本数（尽量平均分配，余数补到最后一个客户端）\n",
    "client_train_sizes = [n_train // 3] * 3\n",
    "remaining_train = n_train % 3\n",
    "if remaining_train > 0:\n",
    "    client_train_sizes[-1] += remaining_train  # 余数加到最后一个客户端\n",
    "\n",
    "# 生成训练集的随机索引（打乱顺序后切分）\n",
    "train_indices = np.random.permutation(n_train)  # 生成0~n_train-1的随机排列\n",
    "current_train = 0\n",
    "client_train_indices = []\n",
    "for size in client_train_sizes:\n",
    "    # 切分随机索引，每个客户端对应一组不重叠的索引\n",
    "    client_train_indices.append(train_indices[current_train:current_train + size])\n",
    "    current_train += size\n",
    "\n",
    "# ---------------------- 测试集随机分配 ----------------------\n",
    "# 计算测试集总样本数\n",
    "n_test = len(global_test_image)  # 原代码中总测试样本为500（可自动适配实际长度）\n",
    "# 计算每个客户端的测试样本数（逻辑同训练集）\n",
    "client_test_sizes = [n_test // 3] * 3\n",
    "remaining_test = n_test % 3\n",
    "if remaining_test > 0:\n",
    "    client_test_sizes[-1] += remaining_test\n",
    "\n",
    "# 生成测试集的随机索引（打乱顺序后切分）\n",
    "test_indices = np.random.permutation(n_test)  # 生成0~n_test-1的随机排列\n",
    "current_test = 0\n",
    "client_test_indices = []\n",
    "for size in client_test_sizes:\n",
    "    client_test_indices.append(test_indices[current_test:current_test + size])\n",
    "    current_test += size\n",
    "\n",
    "# ---------------------- 为每个客户端分配数据 ----------------------\n",
    "for i in range(3):\n",
    "    # 训练集：使用当前客户端的随机索引\n",
    "    train_idx = client_train_indices[i]\n",
    "    train_data = MultimodalDataset(\n",
    "        image_data=train_image[train_idx],  # 按随机索引取数据\n",
    "        text_data=train_text[train_idx],\n",
    "        vector_data=train_vector[train_idx],\n",
    "        targets=train_target[train_idx],\n",
    "        is_test=False\n",
    "    )\n",
    "    client_train_datasets.append(train_data)\n",
    "\n",
    "    # 测试集：使用当前客户端的随机索引\n",
    "    test_idx = client_test_indices[i]\n",
    "    test_data = MultimodalDataset(\n",
    "        image_data=global_test_image[test_idx],\n",
    "        text_data=global_test_text[test_idx],\n",
    "        vector_data=global_test_vector[test_idx],\n",
    "        targets=global_test_target[test_idx],\n",
    "        is_test=True\n",
    "    )\n",
    "    client_test_datasets.append(test_data)\n",
    "    print(f\"客户端 {i+1} - 训练样本: {len(train_data)}, 测试样本: {len(test_data)}\")\n",
    "\n",
    "# 全局测试集保持不变\n",
    "global_test_dataset = MultimodalDataset(\n",
    "    image_data=global_test_image,\n",
    "    text_data=global_test_text,\n",
    "    vector_data=global_test_vector,\n",
    "    targets=global_test_target,\n",
    "    is_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595198b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始联邦训练...\n",
      "\n",
      "===== 联邦轮次 1/5 =====\n",
      "客户端 1 平均损失: 13.8021\n",
      "客户端 2 平均损失: 13.6297\n",
      "客户端 3 平均损失: 13.5192\n",
      "\n",
      "===== 联邦轮次 2/5 =====\n",
      "客户端 1 平均损失: 1.0998\n",
      "客户端 2 平均损失: 1.2401\n",
      "客户端 3 平均损失: 1.2195\n",
      "\n",
      "===== 联邦轮次 3/5 =====\n",
      "客户端 1 平均损失: 0.9299\n",
      "客户端 2 平均损失: 1.0363\n",
      "客户端 3 平均损失: 1.1272\n",
      "\n",
      "===== 联邦轮次 4/5 =====\n",
      "客户端 1 平均损失: 0.8900\n",
      "客户端 2 平均损失: 0.9812\n",
      "客户端 3 平均损失: 0.8628\n",
      "\n",
      "===== 联邦轮次 5/5 =====\n",
      "客户端 1 平均损失: 0.7850\n",
      "客户端 2 平均损失: 0.8428\n",
      "客户端 3 平均损失: 0.7433\n",
      "\n",
      "提取客户端特征...\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和联邦系统\n",
    "img_encoder = ImageEncoder(output_dim=128)\n",
    "text_encoder = TextEncoder(input_dim=50, output_dim=128)\n",
    "vec_encoder = VectorEncoder(input_dim=32, output_dim=128)\n",
    "base_model = MultimodalEncoder(img_encoder, text_encoder, vec_encoder, latent_dim=128, output_dim=1)\n",
    "\n",
    "server = Server(base_model, num_clients=3)\n",
    "for i in range(3):\n",
    "    client = Client(\n",
    "        client_id=i+1,\n",
    "        model=base_model,\n",
    "        train_dataset=client_train_datasets[i],\n",
    "        test_dataset=client_test_datasets[i]\n",
    "    )\n",
    "    server.add_client(client)\n",
    "\n",
    "# 联邦训练\n",
    "print(\"\\n开始联邦训练...\")\n",
    "trained_global_model = server.federated_train(rounds=5, global_test_dataset=global_test_dataset)\n",
    "\n",
    "# 提取所有客户端的训练集+测试集特征（使用全局模型）\n",
    "print(\"\\n提取客户端特征...\")\n",
    "all_train_feats, all_test_feats = server.get_all_client_features(use_global_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cda8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 训练集特征与标签 =====\n",
      "客户端 1 训练集:\n",
      "  样本数: 666\n",
      "  特征形状: (666, 128)\n",
      "客户端 2 训练集:\n",
      "  样本数: 666\n",
      "  特征形状: (666, 128)\n",
      "客户端 3 训练集:\n",
      "  样本数: 668\n",
      "  特征形状: (668, 128)\n",
      "===== 测试集特征与预测 =====\n",
      "客户端 1 测试集:\n",
      "  样本数: 166\n",
      "  特征形状: (166, 128)\n",
      "客户端 2 测试集:\n",
      "  样本数: 166\n",
      "  特征形状: (166, 128)\n",
      "客户端 3 测试集:\n",
      "  样本数: 168\n",
      "  特征形状: (168, 128)\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集特征与标签\n",
    "print(\"\\n===== 训练集特征与标签 =====\")\n",
    "for tf in all_train_feats:\n",
    "    client_id = tf['client_id']\n",
    "    print(f\"客户端 {client_id} 训练集:\")\n",
    "    print(f\"  样本数: {len(tf['train_indices'])}\")\n",
    "    print(f\"  特征形状: {tf['train_features'].shape}\")\n",
    "    #print(f\"  第1个样本特征前5维: {tf['train_features'][0, :5].round(4)}\")\n",
    "    #print(f\"  第1个样本标签: {tf['train_targets'][0].round(4)}\\n\")  # 训练集标签\n",
    "\n",
    "# 输出测试集特征\n",
    "print(\"===== 测试集特征与预测 =====\")\n",
    "for tf in all_test_feats:\n",
    "    client_id = tf['client_id']\n",
    "    print(f\"客户端 {client_id} 测试集:\")\n",
    "    print(f\"  样本数: {len(tf['test_indices'])}\")\n",
    "    print(f\"  特征形状: {tf['test_features'].shape}\")\n",
    "    #print(f\"  第1个样本特征前5维: {tf['test_features'][0, :5].round(4)}\")\n",
    "    #print(f\"  第1个样本预测值: {tf['test_preds'][0].round(4)}\")\n",
    "    #print(f\"  第1个样本真实标签: {tf['test_targets'][0].round(4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01644f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 模型训练与评估 =====\n",
      "处理客户端 1...\n",
      "处理客户端 2...\n",
      "处理客户端 3...\n",
      "\n",
      "===== 各客户端模型评估结果汇总 =====\n",
      "   客户端ID  训练样本数  测试样本数  特征维度  均方误差(MSE)\n",
      "0      1    666    166   128   0.408163\n",
      "1      2    666    166   128   0.403993\n",
      "2      3    668    168   128   0.364085\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 存储各客户端的评估结果\n",
    "results = []\n",
    "\n",
    "# 遍历每个客户端进行模型训练与评估\n",
    "print(\"\\n===== 模型训练与评估 =====\")\n",
    "for i in range(len(all_train_feats)):\n",
    "    # 获取当前客户端的训练数据和测试数据\n",
    "    train_data = all_train_feats[i]\n",
    "    test_data = all_test_feats[i]\n",
    "    client_id = train_data['client_id']\n",
    "    \n",
    "    print(f\"处理客户端 {client_id}...\")\n",
    "    \n",
    "    # 提取训练特征和标签\n",
    "    X_train = train_data['train_features']\n",
    "    y_train = train_data['train_targets']\n",
    "    \n",
    "    # 提取测试特征和真实标签\n",
    "    X_test = test_data['test_features']\n",
    "    y_test = test_data['test_targets']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    # 初始化并训练随机森林模型\n",
    "    rf_model =RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # 计算均方误差\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # 存储结果\n",
    "    results.append({\n",
    "        '客户端ID': client_id,\n",
    "        '训练样本数': len(train_data['train_indices']),\n",
    "        '测试样本数': len(test_data['test_indices']),\n",
    "        '特征维度': X_train.shape[1],\n",
    "        '均方误差(MSE)': round(mse, 6)\n",
    "    })\n",
    "    \n",
    "    # 保存预测结果到测试数据中（如果需要）\n",
    "    test_data['test_preds'] = y_pred\n",
    "\n",
    "# 汇总结果为表格\n",
    "results_df1 = pd.DataFrame(results)\n",
    "\n",
    "# 输出汇总表格\n",
    "print(\"\\n===== 各客户端模型评估结果汇总 =====\")\n",
    "print(results_df1)\n",
    "\n",
    "# 可选：将结果保存为CSV文件\n",
    "# results_df1.to_csv('client_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cb778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 基线对比：仅 VAE（+ 您的方法 占位）=====\n",
      "\n",
      "===== 两方法对比（仅你要求的列） =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>客户端ID</th>\n",
       "      <th>训练样本数</th>\n",
       "      <th>测试样本数</th>\n",
       "      <th>VAE潜维度</th>\n",
       "      <th>VAE降维RF-MSE</th>\n",
       "      <th>您的方法-MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>166</td>\n",
       "      <td>16</td>\n",
       "      <td>0.770507</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>666</td>\n",
       "      <td>166</td>\n",
       "      <td>16</td>\n",
       "      <td>0.792421</td>\n",
       "      <td>0.403993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>668</td>\n",
       "      <td>168</td>\n",
       "      <td>16</td>\n",
       "      <td>1.065883</td>\n",
       "      <td>0.364085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   客户端ID  训练样本数  测试样本数  VAE潜维度  VAE降维RF-MSE  您的方法-MSE\n",
       "0      1    666    166      16     0.770507  0.408163\n",
       "1      2    666    166      16     0.792421  0.403993\n",
       "2      3    668    168      16     1.065883  0.364085"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== 基线对比：仅 VAE + 您的方法（回填） ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# 工具：拼接多模态 → 二维特征\n",
    "# -------------------------------\n",
    "def flatten_and_concat(image, text, vector):\n",
    "    Xi = image.reshape(image.shape[0], -1)   # e.g. 3*32*32\n",
    "    Xt = text.reshape(text.shape[0], -1)     # e.g. 10*50\n",
    "    Xv = vector                              # e.g. 32\n",
    "    return np.hstack([Xi, Xt, Xv])\n",
    "\n",
    "# -------------------------------\n",
    "# 最简 VAE（β-VAE 可用 beta>1）\n",
    "# -------------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=16):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(64, z_dim)\n",
    "        self.logvar = nn.Linear(64, z_dim)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, in_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        x_hat = self.dec(z)\n",
    "        return x_hat, mu, logvar, z\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar, beta=1.0):\n",
    "    recon = F.mse_loss(x_hat, x, reduction='mean')\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + beta * kld\n",
    "\n",
    "# -------------------------------\n",
    "# 训练 VAE -> 抽取潜变量 z -> RF 回归\n",
    "# -------------------------------\n",
    "def train_rf_with_vae(train_data, test_data,\n",
    "                      z_dim=16, beta=1.0, epochs=80, batch_size=256, lr=1e-3,\n",
    "                      n_estimators=5, random_state=0):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # 1) 组装特征并标准化（按训练集拟合）\n",
    "    X_tr_raw = flatten_and_concat(train_data.image_data, train_data.text_data, train_data.vector_data)\n",
    "    X_te_raw = flatten_and_concat(test_data.image_data,  test_data.text_data,  test_data.vector_data)\n",
    "    scaler = StandardScaler().fit(X_tr_raw)\n",
    "    X_tr = scaler.transform(X_tr_raw).astype(np.float32)\n",
    "    X_te = scaler.transform(X_te_raw).astype(np.float32)\n",
    "\n",
    "    # 2) 训练 VAE（仅用训练集）\n",
    "    vae = VAE(in_dim=X_tr.shape[1], z_dim=z_dim).to(device)\n",
    "    opt = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "    dl = DataLoader(TensorDataset(torch.from_numpy(X_tr)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    vae.train()\n",
    "    for _ in range(epochs):\n",
    "        for (xb,) in dl:\n",
    "            xb = xb.to(device)\n",
    "            xh, mu, logv, _ = vae(xb)\n",
    "            loss = vae_loss(xb, xh, mu, logv, beta)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # 3) 抽取潜变量（使用 mu 作为稳定特征）\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        Ztr = []\n",
    "        for (xb,) in DataLoader(TensorDataset(torch.from_numpy(X_tr)), batch_size=4096, shuffle=False):\n",
    "            xb = xb.to(device)\n",
    "            _, mu, _, _ = vae(xb)\n",
    "            Ztr.append(mu.cpu())\n",
    "        Ztr = torch.cat(Ztr, dim=0).numpy()\n",
    "\n",
    "        Zte = []\n",
    "        for (xb,) in DataLoader(TensorDataset(torch.from_numpy(X_te)), batch_size=4096, shuffle=False):\n",
    "            xb = xb.to(device)\n",
    "            _, mu, _, _ = vae(xb)\n",
    "            Zte.append(mu.cpu())\n",
    "        Zte = torch.cat(Zte, dim=0).numpy()\n",
    "\n",
    "    # 4) 用 RF 做回归并评估\n",
    "    y_train, y_test = train_data.targets, test_data.targets\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_features='auto',\n",
    "                               n_jobs=-1, random_state=random_state)\n",
    "    rf.fit(Ztr, y_train)\n",
    "    y_pred = rf.predict(Zte)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse, {\"z_dim\": z_dim}\n",
    "\n",
    "# -------------------------------\n",
    "# 主流程（只输出 VAE 与 您的方法 占位）\n",
    "# -------------------------------\n",
    "comparison_rows = []\n",
    "print(\"===== 基线对比：仅 VAE（+ 您的方法 占位）=====\")\n",
    "for client_idx in range(3):\n",
    "    client_id = client_idx + 1\n",
    "    train_data = client_train_datasets[client_idx]\n",
    "    test_data  = client_test_datasets[client_idx]\n",
    "\n",
    "    vae_mse, vae_info = train_rf_with_vae(\n",
    "        train_data, test_data,\n",
    "        z_dim=16, beta=1.0, epochs=80, batch_size=256, lr=1e-3,\n",
    "        n_estimators=5, random_state=0\n",
    "    )\n",
    "\n",
    "    row = {\n",
    "        '客户端ID': client_id,\n",
    "        '训练样本数': len(train_data),\n",
    "        '测试样本数': len(test_data),\n",
    "        'VAE潜维度': vae_info['z_dim'],\n",
    "        'VAE降维RF-MSE': round(float(vae_mse), 6),\n",
    "        '您的方法-MSE': None  # 由你外部结果回填\n",
    "    }\n",
    "    comparison_rows.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(comparison_rows)\n",
    "\n",
    "# 若你已有 “您的方法” 的 MSE 序列，可在此回填（例如来自 results_df1 的最后一列）\n",
    "try:\n",
    "    your_method_mses = results_df1.iloc[:, -1]\n",
    "    for i in range(len(results_df)):\n",
    "        results_df.loc[i, '您的方法-MSE'] = round(float(your_method_mses[i]), 6)\n",
    "except Exception:\n",
    "    pass  # 没有就留空，后续再回填\n",
    "\n",
    "print(\"\\n===== 两方法对比（仅你要求的列） =====\")\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSumMoon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
