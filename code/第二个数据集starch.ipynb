{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd4ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Anaconda3\\envs\\DeepSumMoon\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 1. 多模态数据集类\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, ts_data=None, vector_data=None, targets=None, is_test=False, device=device):\n",
    "        self.ts_data = ts_data  # (N, K, D)\n",
    "        self.vector_data = vector_data  # (N, M)\n",
    "        self.targets = targets  # 标签\n",
    "        self.is_test = is_test\n",
    "        self.device = device\n",
    "        \n",
    "        # 数据标准化\n",
    "        if ts_data is not None:\n",
    "            self.ts_mean = np.mean(ts_data, axis=(0, 1), keepdims=True)\n",
    "            self.ts_std = np.std(ts_data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "            self.ts_data = (ts_data - self.ts_mean) / self.ts_std\n",
    "            self.num_samples = len(ts_data)\n",
    "        if vector_data is not None:\n",
    "            self.vec_mean = np.mean(vector_data, axis=0, keepdims=True)\n",
    "            self.vec_std = np.std(vector_data, axis=0, keepdims=True) + 1e-8\n",
    "            self.vector_data = (vector_data - self.vec_mean) / self.vec_std\n",
    "            if self.ts_data is None:\n",
    "                self.num_samples = len(vector_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'index': idx}\n",
    "        if self.ts_data is not None:\n",
    "            sample['ts'] = torch.tensor(self.ts_data[idx], dtype=torch.float32).to(self.device)\n",
    "        if self.vector_data is not None:\n",
    "            sample['vector'] = torch.tensor(self.vector_data[idx], dtype=torch.float32).to(self.device)\n",
    "        if self.targets is not None:\n",
    "            sample['target'] = torch.tensor(self.targets[idx], dtype=torch.float32).to(self.device)\n",
    "        return sample\n",
    "\n",
    "\n",
    "# 2. 增强版模态编码器\n",
    "class TSEncoder(nn.Module):\n",
    "    \"\"\"增强版时间序列编码器：多尺度特征提取+残差连接\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=128):\n",
    "        super().__init__()\n",
    "        # 多尺度卷积+LSTM融合\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 双向LSTM+残差连接\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden_dim*2,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 时间注意力+门控机制\n",
    "        self.time_attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.gate = nn.Linear(hidden_dim*2 + 128, hidden_dim*2)  # 融合卷积与LSTM特征\n",
    "        \n",
    "        # 输出投影\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, K, D) -> 卷积需要 (batch, D, K)\n",
    "        conv_in = x.permute(0, 2, 1)  # 维度转换\n",
    "        conv1_out = self.conv1(conv_in)  # (batch, 64, K)\n",
    "        conv2_out = self.conv2(conv1_out)  # (batch, 128, K)\n",
    "        conv_feat = conv2_out.permute(0, 2, 1)  # (batch, K, 128)\n",
    "        \n",
    "        # LSTM特征提取\n",
    "        lstm1_out, _ = self.lstm1(conv_feat)  # (batch, K, 2*hidden)\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out + conv_feat)  # 残差连接\n",
    "        \n",
    "        # 时间注意力\n",
    "        attn_weights = F.softmax(self.time_attn(lstm2_out).squeeze(-1), dim=1)  # (batch, K)\n",
    "        attn_feat = torch.bmm(attn_weights.unsqueeze(1), lstm2_out).squeeze(1)  # (batch, 2*hidden)\n",
    "        \n",
    "        # 门控融合卷积全局特征与注意力特征\n",
    "        global_conv_feat = torch.mean(conv_feat, dim=1)  # (batch, 128)\n",
    "        gate_input = torch.cat([attn_feat, global_conv_feat], dim=1)\n",
    "        gate = torch.sigmoid(self.gate(gate_input))\n",
    "        fused = gate * attn_feat + (1 - gate) * global_conv_feat\n",
    "        \n",
    "        return self.fc(self.dropout(fused))\n",
    "\n",
    "\n",
    "class VectorEncoder(nn.Module):\n",
    "    \"\"\"增强版向量编码器：深度MLP+特征交互\"\"\"\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super().__init__()\n",
    "        # 深度MLP+残差\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 特征交互层（捕捉特征间非线性关系）\n",
    "        self.interact = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 残差连接\n",
    "        out = self.layer1(x)\n",
    "        residual = out\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out) + residual  # 残差连接\n",
    "        \n",
    "        # 特征交互\n",
    "        interacted = self.interact(out) * out  # 元素级交互\n",
    "        return self.fc(self.dropout(interacted))\n",
    "\n",
    "\n",
    "# 3. 增强版多模态融合模型\n",
    "class MultimodalEncoder(nn.Module):\n",
    "    def __init__(self, ts_encoder, vector_encoder, latent_dim=128, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.ts_encoder = ts_encoder\n",
    "        self.vector_encoder = vector_encoder\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 动态模态注意力（替代固定权重）\n",
    "        self.modal_attn = nn.Sequential(\n",
    "            nn.Linear(128*2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 交叉注意力（双向交互）\n",
    "        self.ts2vec_attn = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)\n",
    "        self.vec2ts_attn = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # 多尺度融合网络\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128*2, 512),  # 更深的融合层\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "        # 改进的预测头（带注意力）\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        \n",
    "        # 特征自注意力（增强全局依赖）\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # 辅助投影\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: dict, return_embedding=False):\n",
    "        batch_size = next(iter(v for k, v in x.items() if k != 'index')).size(0)\n",
    "        \n",
    "        # 提取基础特征\n",
    "        ts_feat = self.ts_encoder(x['ts']) if 'ts' in x else torch.zeros(batch_size, 128).to(device)\n",
    "        vec_feat = self.vector_encoder(x['vector']) if 'vector' in x else torch.zeros(batch_size, 128).to(device)\n",
    "        \n",
    "        # 双向交叉注意力（模态间深度交互）\n",
    "        ts_expand = ts_feat.unsqueeze(1)  # (B,1,128)\n",
    "        vec_expand = vec_feat.unsqueeze(1)\n",
    "        \n",
    "        vec_attended_by_ts, _ = self.ts2vec_attn(vec_expand, ts_expand, ts_expand)  # 向量参考时间序列\n",
    "        ts_attended_by_vec, _ = self.vec2ts_attn(ts_expand, vec_expand, vec_expand)  # 时间序列参考向量\n",
    "        \n",
    "        vec_attended = vec_attended_by_ts.squeeze(1)  # (B,128)\n",
    "        ts_attended = ts_attended_by_vec.squeeze(1)   # (B,128)\n",
    "        \n",
    "        # 动态模态权重（基于当前样本特征自适应）\n",
    "        modal_cat = torch.cat([ts_attended, vec_attended], dim=1)  # (B,256)\n",
    "        weights = self.modal_attn(modal_cat)  # (B,2)\n",
    "        ts_weighted = ts_attended * weights[:, 0:1]  # (B,128)\n",
    "        vec_weighted = vec_attended * weights[:, 1:2]  # (B,128)\n",
    "        \n",
    "        # 融合特征\n",
    "        fused = torch.cat([ts_weighted, vec_weighted], dim=1)  # (B,256)\n",
    "        unified_embedding = F.relu(self.fusion(fused))  # (B, latent_dim)\n",
    "        \n",
    "        # 自注意力增强全局特征\n",
    "        attn_emb, _ = self.self_attn(\n",
    "            unified_embedding.unsqueeze(1),\n",
    "            unified_embedding.unsqueeze(1),\n",
    "            unified_embedding.unsqueeze(1)\n",
    "        )\n",
    "        unified_embedding = attn_emb.squeeze(1) + unified_embedding  # 残差\n",
    "        \n",
    "        # 预测输出\n",
    "        output = self.regressor(unified_embedding)\n",
    "        projection = self.projection(unified_embedding)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return output, projection, unified_embedding\n",
    "        return output, projection\n",
    "\n",
    "\n",
    "# 4. 增强版联邦客户端（修复对比损失批次不匹配问题）\n",
    "class Client:\n",
    "    def __init__(self, client_id, model, train_dataset, test_dataset=None, learning_rate=0.0005):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model).to(device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        self.train_feature_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) if test_dataset else None\n",
    "        \n",
    "        # 优化器改进：带动量的AdamW\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-5,  # L2正则化\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(  # 学习率调度\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2, verbose=False\n",
    "        )\n",
    "        self.local_epochs = 5  # 增加本地训练轮次\n",
    "        \n",
    "        # 改进的互信息映射（更深层）\n",
    "        self.f_map = nn.Sequential(\n",
    "            nn.Linear(model.latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        for param in self.f_map.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 保留核心损失函数，优化数值稳定性\n",
    "    def compute_mi_loss_Y_Z(self, Z, Y):\n",
    "        f_Z = self.f_map(Z)\n",
    "        f_Z_norm = (f_Z - f_Z.mean(dim=0)) / (f_Z.std(dim=0) + 1e-8)\n",
    "        Y_norm = (Y - Y.mean(dim=0)) / (Y.std(dim=0) + 1e-8)\n",
    "        cov = torch.cov(torch.cat([f_Z_norm, Y_norm], dim=1).T)[0, 1]\n",
    "        corr = cov / (torch.std(f_Z_norm) * torch.std(Y_norm) + 1e-8)\n",
    "        return -torch.log(torch.clamp(torch.abs(corr), min=1e-6))  # 避免log(0)\n",
    "\n",
    "    def compute_kl_loss_modal(self, modal_feats):\n",
    "        if len(modal_feats) < 2:\n",
    "            return 0.0\n",
    "        kl_total = 0.0\n",
    "        num_pairs = 0\n",
    "        var = nn.Parameter(torch.tensor(0.1)).to(device)\n",
    "        dist_list = [torch.distributions.Normal(feat.mean(dim=0), var) for feat in modal_feats]  # 更稳定的分布估计\n",
    "        \n",
    "        for i in range(len(dist_list)):\n",
    "            for j in range(i+1, len(dist_list)):\n",
    "                kl_ij = torch.distributions.kl.kl_divergence(dist_list[i], dist_list[j]).mean()\n",
    "                kl_ji = torch.distributions.kl.kl_divergence(dist_list[j], dist_list[i]).mean()\n",
    "                kl_total += (kl_ij + kl_ji) / 2\n",
    "                num_pairs += 1\n",
    "        return kl_total / (num_pairs + 1e-8)\n",
    "\n",
    "    def compute_contrastive_loss_anti_forget(self, current_Z, prev_global_Z, history_global_Zs, temperature=0.3):\n",
    "        \"\"\"修复批次大小不匹配问题：确保正负样本与当前批次大小一致\"\"\"\n",
    "        batch_size = current_Z.size(0)  # 获取当前批次的实际大小\n",
    "        current_Z_norm = F.normalize(current_Z, dim=1)\n",
    "        \n",
    "        # 调整正样本大小与当前批次一致\n",
    "        prev_global_Z_norm = F.normalize(prev_global_Z, dim=1)\n",
    "        if prev_global_Z_norm.size(0) > batch_size:\n",
    "            pos_samples = prev_global_Z_norm[:batch_size]  # 截断到当前批次大小\n",
    "        elif prev_global_Z_norm.size(0) < batch_size:\n",
    "            # 不足时重复填充（保持数据分布）\n",
    "            repeat_times = (batch_size // prev_global_Z_norm.size(0)) + 1\n",
    "            pos_samples = torch.repeat_interleave(prev_global_Z_norm, repeat_times, dim=0)[:batch_size]\n",
    "        else:\n",
    "            pos_samples = prev_global_Z_norm\n",
    "        \n",
    "        # 调整负样本大小与当前批次一致\n",
    "        if history_global_Zs:\n",
    "            neg_candidates = torch.cat([F.normalize(emb, dim=1) for emb in history_global_Zs], dim=0)\n",
    "            # 过滤高相似度负样本\n",
    "            sim_matrix = torch.matmul(current_Z_norm, neg_candidates.T)\n",
    "            mask = sim_matrix < 0.5  # 只保留相似度<0.5的负样本\n",
    "            \n",
    "            neg_samples = []\n",
    "            for i in range(batch_size):\n",
    "                neg_i = neg_candidates[mask[i]]\n",
    "                if neg_i.size(0) == 0:\n",
    "                    neg_i = neg_candidates[torch.randperm(neg_candidates.size(0))[:1]]  # 兜底\n",
    "                # 确保每个样本有一个负样本\n",
    "                neg_samples.append(neg_i[0:1])  # 取第一个符合条件的负样本\n",
    "            neg_samples = torch.cat(neg_samples, dim=0)  # 形状：(batch_size, latent_dim)\n",
    "        else:\n",
    "            # 无历史时，使用当前样本的随机排列作为负样本\n",
    "            neg_samples = current_Z_norm[torch.randperm(batch_size)]\n",
    "        \n",
    "        # 计算相似度并拼接（此时批次大小已一致）\n",
    "        pos_sim = torch.matmul(current_Z_norm, pos_samples.T).diag() / temperature  # (batch_size,)\n",
    "        neg_sim = torch.matmul(current_Z_norm, neg_samples.T).diag() / temperature  # (batch_size,)\n",
    "        logits = torch.stack([pos_sim, neg_sim], dim=1)  # (batch_size, 2)\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(device)  # 正样本标签为0\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def local_train(self, global_model, prev_global_Z, history_global_Zs, mu=1e-3):\n",
    "        self.model.load_state_dict(global_model.state_dict())\n",
    "        self.model.train()\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.local_epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_dataloader:\n",
    "                current_pred, _, current_Z = self.model(batch, return_embedding=True)\n",
    "                \n",
    "                # 收集模态特征\n",
    "                modal_feats = []\n",
    "                if 'ts' in batch:\n",
    "                    modal_feats.append(self.model.ts_encoder(batch['ts']))\n",
    "                if 'vector' in batch:\n",
    "                    modal_feats.append(self.model.vector_encoder(batch['vector']))\n",
    "                \n",
    "                # 损失组合（动态权重）\n",
    "                reg_loss = F.mse_loss(current_pred.squeeze(), batch['target'])\n",
    "                mi_loss = self.compute_mi_loss_Y_Z(current_Z, batch['target'].unsqueeze(1))\n",
    "                kl_loss = self.compute_kl_loss_modal(modal_feats) if modal_feats else 0\n",
    "                contrast_loss = self.compute_contrastive_loss_anti_forget(\n",
    "                    current_Z, prev_global_Z, history_global_Zs\n",
    "                )\n",
    "                \n",
    "                # 动态调整损失权重（根据训练阶段）\n",
    "                if epoch < self.local_epochs // 2:  # 前期侧重拟合\n",
    "                    loss = reg_loss + mu * (mi_loss + 0.5*kl_loss + 0.5*contrast_loss)\n",
    "                else:  # 后期侧重泛化\n",
    "                    loss = reg_loss + mu * (0.5*mi_loss + kl_loss + contrast_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)  # 梯度裁剪防爆炸\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(self.train_dataloader)\n",
    "            self.scheduler.step(avg_loss)  # 学习率调度\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss  # 记录最优损失\n",
    "        \n",
    "        print(f\"客户端 {self.client_id} 最佳平均损失: {best_loss:.4f}\")\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    # 特征提取接口\n",
    "    def extract_train_features(self, use_global_model=False, global_model=None):\n",
    "        model = global_model if (use_global_model and global_model) else self.model\n",
    "        model.eval()\n",
    "        all_features = []\n",
    "        all_indices = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.train_feature_dataloader:\n",
    "                _, _, emb = model(batch, return_embedding=True)\n",
    "                all_features.append(emb.cpu())\n",
    "                all_indices.extend(batch['index'].cpu().numpy())\n",
    "                all_targets.extend(batch['target'].cpu().numpy())\n",
    "        \n",
    "        model.train()\n",
    "        features_np = torch.cat(all_features, dim=0).numpy()\n",
    "        targets_np = np.array(all_targets)\n",
    "        \n",
    "        return {\n",
    "            'client_id': self.client_id,\n",
    "            'train_indices': all_indices,\n",
    "            'train_features': features_np,\n",
    "            'train_targets': targets_np\n",
    "        }\n",
    "\n",
    "    def extract_test_features(self, use_global_model=False, global_model=None):\n",
    "        if not self.test_dataset:\n",
    "            return None\n",
    "        \n",
    "        model = global_model if (use_global_model and global_model) else self.model\n",
    "        model.eval()\n",
    "        all_features = []\n",
    "        all_indices = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_dataloader:\n",
    "                pred, _, emb = model(batch, return_embedding=True)\n",
    "                all_features.append(emb.cpu())\n",
    "                all_indices.extend(batch['index'].cpu().numpy())\n",
    "                all_preds.append(pred.squeeze().cpu().numpy())\n",
    "                if 'target' in batch:\n",
    "                    all_targets.extend(batch['target'].cpu().numpy())\n",
    "        \n",
    "        model.train()\n",
    "        features_np = torch.cat(all_features, dim=0).numpy()\n",
    "        preds_np = np.concatenate(all_preds, axis=0)\n",
    "        targets_np = np.array(all_targets) if all_targets else None\n",
    "        \n",
    "        return {\n",
    "            'client_id': self.client_id,\n",
    "            'test_indices': all_indices,\n",
    "            'test_features': features_np,\n",
    "            'test_preds': preds_np,\n",
    "            'test_targets': targets_np\n",
    "        }\n",
    "\n",
    "\n",
    "# 5. 增强版服务器\n",
    "class Server:\n",
    "    def __init__(self, model, num_clients):\n",
    "        self.global_model = copy.deepcopy(model).to(device)\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.history_global_embs = []\n",
    "        self.latent_dim = model.latent_dim\n",
    "        self.client_weights = [1.0/num_clients for _ in range(num_clients)]  # 客户端权重\n",
    "\n",
    "    def add_client(self, client):\n",
    "        self.clients.append(client)\n",
    "\n",
    "    def aggregate_parameters(self, client_params_list):\n",
    "        \"\"\"加权聚合（修复num_batches_tracked的类型错误）\"\"\"\n",
    "        aggregated_params = {\n",
    "            name: torch.zeros_like(param) \n",
    "            for name, param in self.global_model.state_dict().items()\n",
    "        }\n",
    "        \n",
    "        for i, params in enumerate(client_params_list):\n",
    "            w = self.client_weights[i]  # 客户端权重\n",
    "            for name, param in params.items():\n",
    "                if 'num_batches_tracked' in name:\n",
    "                    # 整数型参数特殊处理：先按权重比例累加整数，再归一化\n",
    "                    int_weight = int(round(w * self.num_clients))\n",
    "                    aggregated_params[name] += param.long() * int_weight\n",
    "                else:\n",
    "                    # 浮点型参数正常加权\n",
    "                    aggregated_params[name] += param.to(aggregated_params[name].dtype) * w\n",
    "        \n",
    "        # 最终处理：整数型参数归一化，浮点型参数平滑\n",
    "        for name in aggregated_params:\n",
    "            if 'num_batches_tracked' in name:\n",
    "                aggregated_params[name] = (aggregated_params[name] // self.num_clients).long()\n",
    "            else:\n",
    "                aggregated_params[name] = 0.9 * aggregated_params[name] + 0.1 * self.global_model.state_dict()[name]\n",
    "        \n",
    "        self.global_model.load_state_dict(aggregated_params)\n",
    "        return self.global_model.state_dict()\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        if test_dataset is None:\n",
    "            return\n",
    "        self.global_model.eval()\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        total_mse = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                pred, _ = self.global_model(batch)\n",
    "                mse = F.mse_loss(pred.squeeze(), batch['target'], reduction='sum')\n",
    "                total_mse += mse.item()\n",
    "                total_samples += batch['target'].size(0)\n",
    "        \n",
    "        rmse = np.sqrt(total_mse / total_samples)\n",
    "        #print(f\"全局模型测试RMSE: {rmse:.4f}\")\n",
    "        self.global_model.train()\n",
    "        return rmse\n",
    "\n",
    "    def federated_train(self, rounds=10, global_test_dataset=None):\n",
    "        for round_idx in range(rounds):\n",
    "            print(f\"\\n===== 联邦轮次 {round_idx + 1}/{rounds} =====\")\n",
    "            \n",
    "            client_params_list = []\n",
    "            test_batch = None  # 初始化test_batch\n",
    "            \n",
    "            # 获取上一轮全局表征\n",
    "            if global_test_dataset:\n",
    "                test_loader = DataLoader(global_test_dataset, batch_size=32, shuffle=False)\n",
    "                test_batch = next(iter(test_loader))\n",
    "                \n",
    "                if self.history_global_embs:\n",
    "                    with torch.no_grad():\n",
    "                        _, _, prev_global_Z = self.global_model(test_batch, return_embedding=True)\n",
    "                    prev_global_Z = prev_global_Z.detach()\n",
    "                    history_global_Zs = [z.detach() for z in self.history_global_embs[-5:]]\n",
    "                else:\n",
    "                    prev_global_Z = torch.zeros(32, self.latent_dim).to(device)\n",
    "                    history_global_Zs = []\n",
    "            else:\n",
    "                prev_global_Z = torch.zeros(32, self.latent_dim).to(device)\n",
    "                history_global_Zs = []\n",
    "            \n",
    "            # 客户端本地训练\n",
    "            client_losses = []\n",
    "            for client in self.clients:\n",
    "                client_params = client.local_train(\n",
    "                    self.global_model, prev_global_Z, history_global_Zs\n",
    "                )\n",
    "                client_params_list.append(client_params)\n",
    "                client_losses.append(1.0)  # 实际应用中可替换为客户端返回的损失\n",
    "            \n",
    "            # 动态调整客户端权重\n",
    "            if round_idx > 0:\n",
    "                inv_loss = [1.0 / (loss + 1e-8) for loss in client_losses]\n",
    "                self.client_weights = [w / sum(inv_loss) for w in inv_loss]\n",
    "            \n",
    "            # 聚合参数\n",
    "            self.aggregate_parameters(client_params_list)\n",
    "            \n",
    "            # 保存全局表征\n",
    "            if global_test_dataset and test_batch is not None:\n",
    "                with torch.no_grad():\n",
    "                    _, _, global_Z = self.global_model(test_batch, return_embedding=True)\n",
    "                self.history_global_embs.append(global_Z.cpu())\n",
    "                if len(self.history_global_embs) > 10:\n",
    "                    self.history_global_embs.pop(0)\n",
    "            \n",
    "            # 评估全局模型\n",
    "            self.evaluate(global_test_dataset)\n",
    "        \n",
    "        return self.global_model\n",
    "\n",
    "    def get_all_client_features(self, use_global_model=True):\n",
    "        all_train_features = []\n",
    "        all_test_features = []\n",
    "        \n",
    "        for client in self.clients:\n",
    "            train_feats = client.extract_train_features(\n",
    "                use_global_model=use_global_model,\n",
    "                global_model=self.global_model if use_global_model else None\n",
    "            )\n",
    "            all_train_features.append(train_feats)\n",
    "            \n",
    "            test_feats = client.extract_test_features(\n",
    "                use_global_model=use_global_model,\n",
    "                global_model=self.global_model if use_global_model else None\n",
    "            )\n",
    "            if test_feats:\n",
    "                all_test_features.append(test_feats)\n",
    "        \n",
    "        return all_train_features, all_test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ba010",
   "metadata": {},
   "source": [
    "# <center>**实际数据分析**<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3841f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv(\"./corn1.csv\").iloc[:,1:]\n",
    "X = np.array(data.iloc[:,:-4]).reshape(len(data),699,1)\n",
    "Z = np.array(data.iloc[:,-4:-1])\n",
    "y = np.array(data.iloc[:,-1])\n",
    "train_ts, global_test_ts, train_vector,global_test_vector,train_target, global_test_target = train_test_split(X,Z,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e45726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数：64\n",
      "测试集样本数：16\n"
     ]
    }
   ],
   "source": [
    "print(f\"训练集样本数：{train_vector.shape[0]}\")  # 输出 64\n",
    "print(f\"测试集样本数：{global_test_ts.shape[0]}\")    # 输出 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e011b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "客户端 1 - 训练样本: 21, 测试样本: 5\n",
      "客户端 2 - 训练样本: 21, 测试样本: 5\n",
      "客户端 3 - 训练样本: 21, 测试样本: 5\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2. 分配客户端数据（两模态）\n",
    "# ----------------------------\n",
    "client_train_datasets = []\n",
    "client_test_datasets = []\n",
    "for i in range(3):\n",
    "    # 训练集\n",
    "    start = i * 21\n",
    "    end = start + 21\n",
    "    train_data = MultimodalDataset(\n",
    "        ts_data=train_ts[start:end],\n",
    "        vector_data=train_vector[start:end],\n",
    "        targets=train_target[start:end],\n",
    "        is_test=False\n",
    "    )\n",
    "    client_train_datasets.append(train_data)\n",
    "\n",
    "    # 测试集\n",
    "    test_start = i * 5\n",
    "    test_end = test_start + 5\n",
    "    test_data = MultimodalDataset(\n",
    "        ts_data=global_test_ts[test_start:test_end],\n",
    "        vector_data=global_test_vector[test_start:test_end],\n",
    "        targets=global_test_target[test_start:test_end],\n",
    "        is_test=True\n",
    "    )\n",
    "    client_test_datasets.append(test_data)\n",
    "    print(f\"客户端 {i+1} - 训练样本: {len(train_data)}, 测试样本: {len(test_data)}\")\n",
    "\n",
    "\n",
    "# 全局测试集\n",
    "global_test_dataset = MultimodalDataset(\n",
    "    ts_data=global_test_ts,\n",
    "    vector_data=global_test_vector,\n",
    "    targets=global_test_target,\n",
    "    is_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c23b36aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始联邦训练...\n",
      "\n",
      "===== 联邦轮次 1/15 =====\n",
      "客户端 1 最佳平均损失: 0.1136\n",
      "客户端 2 最佳平均损失: 0.0961\n",
      "客户端 3 最佳平均损失: 0.1056\n",
      "\n",
      "===== 联邦轮次 2/15 =====\n",
      "客户端 1 最佳平均损失: 0.0279\n",
      "客户端 2 最佳平均损失: 0.0135\n",
      "客户端 3 最佳平均损失: 0.0175\n",
      "\n",
      "===== 联邦轮次 3/15 =====\n",
      "客户端 1 最佳平均损失: 0.0244\n",
      "客户端 2 最佳平均损失: 0.0171\n",
      "客户端 3 最佳平均损失: 0.0232\n",
      "\n",
      "===== 联邦轮次 4/15 =====\n",
      "客户端 1 最佳平均损失: 0.0158\n",
      "客户端 2 最佳平均损失: 0.0127\n",
      "客户端 3 最佳平均损失: 0.0145\n",
      "\n",
      "===== 联邦轮次 5/15 =====\n",
      "客户端 1 最佳平均损失: 0.0107\n",
      "客户端 2 最佳平均损失: 0.0101\n",
      "客户端 3 最佳平均损失: 0.0143\n",
      "\n",
      "===== 联邦轮次 6/15 =====\n",
      "客户端 1 最佳平均损失: 0.0118\n",
      "客户端 2 最佳平均损失: 0.0104\n",
      "客户端 3 最佳平均损失: 0.0129\n",
      "\n",
      "===== 联邦轮次 7/15 =====\n",
      "客户端 1 最佳平均损失: 0.0121\n",
      "客户端 2 最佳平均损失: 0.0080\n",
      "客户端 3 最佳平均损失: 0.0107\n",
      "\n",
      "===== 联邦轮次 8/15 =====\n",
      "客户端 1 最佳平均损失: 0.0133\n",
      "客户端 2 最佳平均损失: 0.0085\n",
      "客户端 3 最佳平均损失: 0.0120\n",
      "\n",
      "===== 联邦轮次 9/15 =====\n",
      "客户端 1 最佳平均损失: 0.0099\n",
      "客户端 2 最佳平均损失: 0.0081\n",
      "客户端 3 最佳平均损失: 0.0130\n",
      "\n",
      "===== 联邦轮次 10/15 =====\n",
      "客户端 1 最佳平均损失: 0.0121\n",
      "客户端 2 最佳平均损失: 0.0093\n",
      "客户端 3 最佳平均损失: 0.0128\n",
      "\n",
      "===== 联邦轮次 11/15 =====\n",
      "客户端 1 最佳平均损失: 0.0145\n",
      "客户端 2 最佳平均损失: 0.0107\n",
      "客户端 3 最佳平均损失: 0.0097\n",
      "\n",
      "===== 联邦轮次 12/15 =====\n",
      "客户端 1 最佳平均损失: 0.0112\n",
      "客户端 2 最佳平均损失: 0.0091\n",
      "客户端 3 最佳平均损失: 0.0103\n",
      "\n",
      "===== 联邦轮次 13/15 =====\n",
      "客户端 1 最佳平均损失: 0.0129\n",
      "客户端 2 最佳平均损失: 0.0113\n",
      "客户端 3 最佳平均损失: 0.0076\n",
      "\n",
      "===== 联邦轮次 14/15 =====\n",
      "客户端 1 最佳平均损失: 0.0134\n",
      "客户端 2 最佳平均损失: 0.0090\n",
      "客户端 3 最佳平均损失: 0.0109\n",
      "\n",
      "===== 联邦轮次 15/15 =====\n",
      "客户端 1 最佳平均损失: 0.0128\n",
      "客户端 2 最佳平均损失: 0.0091\n",
      "客户端 3 最佳平均损失: 0.0092\n",
      "\n",
      "提取客户端特征...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3. 初始化两模态联邦模型并训练\n",
    "# ----------------------------\n",
    "# 初始化编码器\n",
    "ts_encoder = TSEncoder(\n",
    "    input_dim=1,    # 时间序列每步特征维度\n",
    "    hidden_dim=64,\n",
    "    output_dim=128\n",
    ")\n",
    "vector_encoder = VectorEncoder(\n",
    "    input_dim=3,   # 向量特征维度\n",
    "    output_dim=128\n",
    ")\n",
    "\n",
    "# 初始化多模态融合模型\n",
    "base_model = MultimodalEncoder(\n",
    "    ts_encoder=ts_encoder,\n",
    "    vector_encoder=vector_encoder,\n",
    "    latent_dim=128,\n",
    "    output_dim=1  # 回归任务\n",
    ")\n",
    "\n",
    "# 初始化服务器和客户端\n",
    "server = Server(base_model, num_clients=3)\n",
    "for i in range(3):\n",
    "    client = Client(\n",
    "        client_id=i+1,\n",
    "        model=base_model,\n",
    "        train_dataset=client_train_datasets[i],\n",
    "        test_dataset=client_test_datasets[i]\n",
    "    )\n",
    "    server.add_client(client)\n",
    "\n",
    "# 联邦训练\n",
    "print(\"\\n开始联邦训练...\")\n",
    "trained_global_model = server.federated_train(rounds=5, global_test_dataset=global_test_dataset)\n",
    "\n",
    "# 提取所有客户端的特征（使用全局模型）\n",
    "print(\"\\n提取客户端特征...\")\n",
    "all_train_feats, all_test_feats = server.get_all_client_features(use_global_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a062ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 联邦特征的模型评估 =====\n",
      "处理客户端 1...\n",
      "处理客户端 2...\n",
      "处理客户端 3...\n",
      "\n",
      "===== 联邦特征评估结果 =====\n",
      "   客户端ID  训练样本数  测试样本数  特征维度  联邦特征-MSE\n",
      "0      1     21      5   128  0.053679\n",
      "1      2     21      5   128  0.025789\n",
      "2      3     21      5   128  0.036576\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. 联邦特征的模型评估\n",
    "# ----------------------------\n",
    "# 存储各客户端的评估结果\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "results = []\n",
    "\n",
    "# 遍历每个客户端进行模型训练与评估\n",
    "print(\"\\n===== 联邦特征的模型评估 =====\")\n",
    "for i in range(len(all_train_feats)):\n",
    "    # 获取当前客户端的训练数据和测试数据\n",
    "    train_data = all_train_feats[i]\n",
    "    test_data = all_test_feats[i]\n",
    "    client_id = train_data['client_id']\n",
    "    \n",
    "    print(f\"处理客户端 {client_id}...\")\n",
    "    \n",
    "    # 提取训练特征和标签\n",
    "    X_train = train_data['train_features']\n",
    "    y_train = train_data['train_targets']\n",
    "    \n",
    "    # 提取测试特征和真实标签\n",
    "    X_test = test_data['test_features']\n",
    "    y_test = test_data['test_targets']\n",
    "    \n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)  # 注意测试集用训练集的scaler\n",
    "    \n",
    "    # 训练随机森林模型\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测与评估\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # 存储结果\n",
    "    results.append({\n",
    "        '客户端ID': client_id,\n",
    "        '训练样本数': len(train_data['train_indices']),\n",
    "        '测试样本数': len(test_data['test_indices']),\n",
    "        '特征维度': X_train.shape[1],\n",
    "        '联邦特征-MSE': round(mse, 6)\n",
    "    })\n",
    "    \n",
    "    # 保存预测结果\n",
    "    test_data['test_preds'] = y_pred\n",
    "\n",
    "# 汇总结果\n",
    "results_df1 = pd.DataFrame(results)\n",
    "print(\"\\n===== 联邦特征评估结果 =====\")\n",
    "print(results_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aabc31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 两模态：PCA / TSVD / RP（+ OURS 占位） =====\n",
      "客户端 1 - PCA后总特征数: 4\n",
      "  时间序列: 1维 (累计贡献率: 0.9692)\n",
      "  向量: 3维 (累计贡献率: 1.0000)\n",
      "  PCA降维RF-MSE: 0.067639\n",
      "\n",
      "客户端 2 - PCA后总特征数: 4\n",
      "  时间序列: 1维 (累计贡献率: 0.9809)\n",
      "  向量: 3维 (累计贡献率: 1.0000)\n",
      "  PCA降维RF-MSE: 0.008323\n",
      "\n",
      "客户端 3 - PCA后总特征数: 4\n",
      "  时间序列: 1维 (累计贡献率: 0.9689)\n",
      "  向量: 3维 (累计贡献率: 1.0000)\n",
      "  PCA降维RF-MSE: 0.044063\n",
      "\n",
      "\n",
      "===== 四方法对比（两模态）=====\n",
      "   客户端ID  训练样本数  测试样本数  PCA总维度  时间序列维度  向量维度  PCA降维RF-MSE  TSVD降维RF-MSE  \\\n",
      "0      1     21      5       4       1     3     0.067639      0.067131   \n",
      "1      2     21      5       4       1     3     0.008323      0.039119   \n",
      "2      3     21      5       4       1     3     0.044063      0.090049   \n",
      "\n",
      "   RP降维RF-MSE  联邦特征-MSE  \n",
      "0    0.076459  0.053679  \n",
      "1    0.037575  0.025789  \n",
      "2    0.021852  0.036576  \n"
     ]
    }
   ],
   "source": [
    "# ===== 两模态：PCA / TSVD / RP + OURS（联邦特征）对比，一次性跑完并汇总 =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 两模态特征的PCA降维与评估（保持你的原实现不变）\n",
    "# ----------------------------\n",
    "def pca_multimodal_data_by_contribution(ts_data, vector_data, contribution=0.9):\n",
    "    \"\"\"对时间序列和向量特征分别做PCA降维\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    modal_dims = {}  # 记录各模态降维后的维度\n",
    "    \n",
    "    # 1) 时间序列特征 PCA（先展平时间步）\n",
    "    flat_ts = ts_data.reshape(ts_data.shape[0], -1)  # (N, K*D)\n",
    "    ts_scaled = scaler.fit_transform(flat_ts)\n",
    "    pca_ts = PCA().fit(ts_scaled)\n",
    "    ts_cum_contrib = np.cumsum(pca_ts.explained_variance_ratio_)\n",
    "    ts_dim = np.argmax(ts_cum_contrib >= contribution) + 1\n",
    "    ts_dim = min(ts_dim, flat_ts.shape[1])  # 避免超过原始维度\n",
    "    pca_ts = PCA(n_components=ts_dim)\n",
    "    ts_pca = pca_ts.fit_transform(ts_scaled)\n",
    "    modal_dims['ts'] = ts_dim\n",
    "    modal_dims['ts_contrib'] = ts_cum_contrib[ts_dim-1]\n",
    "    \n",
    "    # 2) 向量特征 PCA\n",
    "    vector_scaled = scaler.fit_transform(vector_data)\n",
    "    pca_vector = PCA().fit(vector_scaled)\n",
    "    vector_cum_contrib = np.cumsum(pca_vector.explained_variance_ratio_)\n",
    "    vector_dim = np.argmax(vector_cum_contrib >= contribution) + 1\n",
    "    vector_dim = min(vector_dim, vector_data.shape[1])\n",
    "    pca_vector = PCA(n_components=vector_dim)\n",
    "    vector_pca = pca_vector.fit_transform(vector_scaled)\n",
    "    modal_dims['vector'] = vector_dim\n",
    "    modal_dims['vector_contrib'] = vector_cum_contrib[vector_dim-1]\n",
    "    \n",
    "    # 总维度和平均贡献率\n",
    "    modal_dims['total'] = ts_dim + vector_dim\n",
    "    modal_dims['total_contrib'] = (ts_dim * modal_dims['ts_contrib'] + \n",
    "                                  vector_dim * modal_dims['vector_contrib']) / modal_dims['total']\n",
    "    \n",
    "    # 拼接所有模态的PCA特征\n",
    "    return np.hstack([ts_pca, vector_pca]), modal_dims\n",
    "\n",
    "def train_rf_with_pca_contribution(train_data, test_data, client_id, contribution=0.9):\n",
    "    # 训练集 PCA 降维（按贡献率）\n",
    "    X_train, modal_dims = pca_multimodal_data_by_contribution(\n",
    "        train_data.ts_data,  # 时间序列\n",
    "        train_data.vector_data,  # 向量\n",
    "        contribution=contribution\n",
    "    )\n",
    "    y_train = train_data.targets\n",
    "    \n",
    "    # 测试集 PCA 降维（复用训练集的 PCA 参数）\n",
    "    scaler = StandardScaler()\n",
    "    # 时间序列\n",
    "    flat_ts_test = test_data.ts_data.reshape(test_data.ts_data.shape[0], -1)\n",
    "    ts_scaled_train = scaler.fit_transform(train_data.ts_data.reshape(train_data.ts_data.shape[0], -1))\n",
    "    pca_ts = PCA(n_components=modal_dims['ts']).fit(ts_scaled_train)\n",
    "    ts_pca_test = pca_ts.transform(scaler.transform(flat_ts_test))\n",
    "    # 向量\n",
    "    vector_scaled_train = scaler.fit_transform(train_data.vector_data)\n",
    "    pca_vector = PCA(n_components=modal_dims['vector']).fit(vector_scaled_train)\n",
    "    vector_pca_test = pca_vector.transform(scaler.transform(test_data.vector_data))\n",
    "    \n",
    "    X_test = np.hstack([ts_pca_test, vector_pca_test])\n",
    "    y_test = test_data.targets\n",
    "    \n",
    "    # 随机森林（与您两模态代码保持一致）\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # 打印各模态降维信息\n",
    "    print(f\"客户端 {client_id} - PCA后总特征数: {modal_dims['total']}\")\n",
    "    print(f\"  时间序列: {modal_dims['ts']}维 (累计贡献率: {modal_dims['ts_contrib']:.4f})\")\n",
    "    print(f\"  向量: {modal_dims['vector']}维 (累计贡献率: {modal_dims['vector_contrib']:.4f})\")\n",
    "    print(f\"  PCA降维RF-MSE: {mse:.6f}\\n\")\n",
    "    \n",
    "    return mse, modal_dims\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TSVD 基线：遵循“每模态独立标准化→训练拟合→测试复用”，维度用训练集奇异值能量达到 contribution 的最小维度\n",
    "# ----------------------------\n",
    "def _tsvd_dim_by_contrib(X_scaled, contribution=0.9):\n",
    "    # 用训练集的奇异值估计累计方差占比\n",
    "    s = np.linalg.svd(X_scaled, full_matrices=False, compute_uv=False)\n",
    "    var = (s**2) / (X_scaled.shape[0] - 1)\n",
    "    ratio = var / var.sum()\n",
    "    cum = np.cumsum(ratio)\n",
    "    dim = int(np.searchsorted(cum, contribution) + 1)\n",
    "    # TruncatedSVD 需要 n_components < min(n_samples, n_features)\n",
    "    max_comp = max(1, min(X_scaled.shape[0], X_scaled.shape[1]) - 1)\n",
    "    dim = max(1, min(dim, max_comp))\n",
    "    return dim\n",
    "\n",
    "def train_rf_with_tsvd_contribution(train_data, test_data, contribution=0.9):\n",
    "    # 时间序列\n",
    "    scaler_ts = StandardScaler()\n",
    "    Xts_tr = scaler_ts.fit_transform(train_data.ts_data.reshape(len(train_data), -1))\n",
    "    dim_ts = _tsvd_dim_by_contrib(Xts_tr, contribution)\n",
    "    tsvd_ts = TruncatedSVD(n_components=dim_ts, random_state=0).fit(Xts_tr)\n",
    "    Xts_te = tsvd_ts.transform(scaler_ts.transform(test_data.ts_data.reshape(len(test_data), -1)))\n",
    "    \n",
    "    # 向量\n",
    "    scaler_v = StandardScaler()\n",
    "    Xv_tr = scaler_v.fit_transform(train_data.vector_data)\n",
    "    dim_v = _tsvd_dim_by_contrib(Xv_tr, contribution)\n",
    "    tsvd_v = TruncatedSVD(n_components=dim_v, random_state=0).fit(Xv_tr)\n",
    "    Xv_te = tsvd_v.transform(scaler_v.transform(test_data.vector_data))\n",
    "    \n",
    "    X_train = np.hstack([tsvd_ts.transform(Xts_tr), tsvd_v.transform(Xv_tr)])\n",
    "    X_test  = np.hstack([Xts_te, Xv_te])\n",
    "    y_train, y_test = train_data.targets, test_data.targets\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# RP 基线（Gaussian）：每模态维度 = PCA 得到的对应模态维度（公平对齐）\n",
    "# ----------------------------\n",
    "def train_rf_with_rp_fixed_dims(train_data, test_data, pca_modal_dims, rp_random_state=0):\n",
    "    # 时间序列\n",
    "    scaler_ts = StandardScaler()\n",
    "    Xts_tr = scaler_ts.fit_transform(train_data.ts_data.reshape(len(train_data), -1))\n",
    "    Xts_te_src = scaler_ts.transform(test_data.ts_data.reshape(len(test_data), -1))\n",
    "    dim_ts = int(pca_modal_dims['ts'])\n",
    "    rp_ts = GaussianRandomProjection(n_components=min(dim_ts, Xts_tr.shape[1]), random_state=rp_random_state).fit(Xts_tr)\n",
    "    Xts_tr_rp = rp_ts.transform(Xts_tr)\n",
    "    Xts_te = rp_ts.transform(Xts_te_src)\n",
    "    \n",
    "    # 向量\n",
    "    scaler_v = StandardScaler()\n",
    "    Xv_tr = scaler_v.fit_transform(train_data.vector_data)\n",
    "    Xv_te_src = scaler_v.transform(test_data.vector_data)\n",
    "    dim_v = int(pca_modal_dims['vector'])\n",
    "    rp_v = GaussianRandomProjection(n_components=min(dim_v, Xv_tr.shape[1]), random_state=rp_random_state).fit(Xv_tr)\n",
    "    Xv_tr_rp = rp_v.transform(Xv_tr)\n",
    "    Xv_te = rp_v.transform(Xv_te_src)\n",
    "    \n",
    "    X_train = np.hstack([Xts_tr_rp, Xv_tr_rp])\n",
    "    X_test  = np.hstack([Xts_te, Xv_te])\n",
    "    y_train, y_test = train_data.targets, test_data.targets\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 主流程：输出你指定的列（两模态版本）\n",
    "# 列顺序：客户端ID | 训练样本数 | 测试样本数 | PCA总维度 | 时间序列维度 | 向量维度 | PCA降维RF-MSE | TSVD降维RF-MSE | RP降维RF-MSE | 联邦特征-MSE\n",
    "# ----------------------------\n",
    "rows = []\n",
    "print(\"===== 两模态：PCA / TSVD / RP（+ OURS 占位） =====\")\n",
    "for client_idx in range(3):\n",
    "    client_id = client_idx + 1\n",
    "    train_data = client_train_datasets[client_idx]\n",
    "    test_data  = client_test_datasets[client_idx]\n",
    "    \n",
    "    # 1) PCA（得到各模态维度 + MSE）\n",
    "    pca_mse, pca_modal_dims = train_rf_with_pca_contribution(\n",
    "        train_data, test_data, client_id=client_id, contribution=0.9\n",
    "    )\n",
    "    # 2) TSVD（训练集奇异值能量 -> 维度）\n",
    "    tsvd_mse = train_rf_with_tsvd_contribution(train_data, test_data, contribution=0.9)\n",
    "    # 3) RP（维度与 PCA 对齐）\n",
    "    rp_mse = train_rf_with_rp_fixed_dims(train_data, test_data, pca_modal_dims=pca_modal_dims, rp_random_state=0)\n",
    "    \n",
    "    rows.append({\n",
    "        '客户端ID': client_id,\n",
    "        '训练样本数': len(train_data),\n",
    "        '测试样本数': len(test_data),\n",
    "        'PCA总维度': int(pca_modal_dims['total']),\n",
    "        '时间序列维度': int(pca_modal_dims['ts']),\n",
    "        '向量维度': int(pca_modal_dims['vector']),\n",
    "        'PCA降维RF-MSE': round(float(pca_mse), 6),\n",
    "        'TSVD降维RF-MSE': round(float(tsvd_mse), 6),\n",
    "        'RP降维RF-MSE': round(float(rp_mse), 6),\n",
    "        '联邦特征-MSE': None  # 后面回填\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# 回填 OURS（联邦特征）的 MSE（若你已有 results_df1）\n",
    "try:\n",
    "    your_method_mses = results_df1['联邦特征-MSE']\n",
    "    for i in range(len(results_df)):\n",
    "        results_df.loc[i, '联邦特征-MSE'] = round(float(your_method_mses.iloc[i]), 6)\n",
    "except Exception:\n",
    "    pass  # 如果没有，就先留空\n",
    "\n",
    "print(\"\\n===== 四方法对比（两模态）=====\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5362c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSumMoon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
